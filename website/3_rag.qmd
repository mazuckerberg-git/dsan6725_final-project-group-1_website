---
title: "Retrieval-Augmented Generation (RAG)"
format:
  html:
    toc: true
    embed-resources: true
---

Within our multi-agent pipeline, we have implemented a Retrieval-Augmented Generation (RAG) system to act as the question-answering portion of the application. The RAG system is designed to read the user's query, retrieve the sources needed in the vectorstore to answer the question, and consolidate the sources to provide an accurate answer. This is especially important for stock market data, as the information is constantly changing, includes exact values, and is often time-sensitive. With a RAG we're able to avoid any type of hallucination and provide the most accurate and up-to-date information to the user.

The RAG system is built using the LangChain framework, which allows us to easily integrate various components such as the vectorstore, retriever, and LLM. The vectorstore was created using FAISS, a popular library for efficient similarity search and clustering of dense vectors. The embeddings used to create the vectorstore were generated from Hugging Face's `intfloat/e5-small-v2` model, which is lightweight and efficient with both semantic understanding and numeric retrieval capabilities. Our retriever is uses the maximum marginal relevance (MMR) algorithm to ensure that the retrieved documents are both relevant and diverse, which is crucial for providing comprehensive answers to user queries. Additionally, we have set k in the retriever to 30 since we found the RAG to suffer in retrieving the correct documents when k was lower, mainly due to the fact the RAG struggles with filtering data by specific dates and names. Lastly, the LLM used for answer generation is Claude 3.5 Sonnet from Anthropic, which is a state-of-the-art model known for its reasoning capabilities and ability to generate coherent and contextually relevant responses with being too wordy.

When it came to creating the structure of the RAG, we also need to take into consideration the chat history and how to best utilize it. We decided that rather than creating a RAG chain, we would seperate it by question answer chain and document retrieval. We first combine the user query and the chat history into a single string, which is then passed to the retriever to find the most relevant documents. The retrieved documents are then passed to the question answer chain, (made up of the llm, system prompt, and documents) to generate a response. This approach allows us to maintain the context of the conversation while ensuring that the RAG system can effectively retrieve and utilize the most relevant information.

The RAG system is able to provide a cohesive answer, either directly based on the retrived documents or a statement that not enough data was found, back to the supervisor agent to be sent to the user. The system is build such that the supervisor agent simple transfers the RAG's output to the user instead of augmenting any additonal details to avoid any type of hallucination.



<br>
<!-- Reference to next section -->
<div style="text-align: right; font-size: 14px;">
  <a href="4_agents.html">Next Section: AI Agents</a>
</div>

---

### References:

${^1}$ [LangChain](https://python.langchain.com/docs/)

${^2}$ [Hugging Face](https://huggingface.co/)

${^3}$ [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/)