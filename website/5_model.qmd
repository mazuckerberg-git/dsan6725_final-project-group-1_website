---
title: "Models"
format:
  html:
    toc: true
    embed-resources: true
---

## Technologies Used

To create our Stock Market LLM Assistant, we utilized a variety of technologies and frameworks. Below is a summary of the key components as well as the decision making process behind our choices.

### LLM Models

Since our project is a multi-agent system, we are using LLMs for all the agents. However, we quickly realized that in order to deliver the best results for our users, we needed to use the best model we had access to for all the agents. Thus, we used **Claude 3.5 Sonnet** for all agents in our system. Claude 3.5 Sonnet is a state-of-the-art LLM that has been trained on a diverse range of data, making it well-suited for a variety of tasks, including code generation, data analysis, and natural language understanding, summarization, and more. Through our testing, we found that other Bedrock models such as Nova and Titan 2 were not as effective for our use cases as Claude 3.5 Sonnet.

At this time, we do not have the resources to use other LLM's such as OpenAI's GPT-4, but that can be a future consideration.

### Embedding Models

Having a good embedding model is crucial for the performance of our RAG agent. We initially used the HuggingFace embedding model `BAAI/bge-small-en` for our RAG agent since we had used it in previous projects for this class. However, we quickly realized that this embedding model was not able to parse both semantic textual data and highly formatted tabular data in the form of json structures. Since our data contains both text data from the stock news articles and tabular data from the stock market data, we needed a model that could handle both to aid in the retrieval process.

After some research, we found that the **HuggingFace embedding model `intfloat/e5-small-v2`** was the best model for our use case. This model, unlike the previous one, is trained for general-purpose retrieval and ranking (multi-task), while `BAAI/bge-small-en` is mainly trained for English text understanding. `intfloat/e5-small-v2` performs best for instruction-following tasks, so our labeled json data was able to be parsed and embedded correctly.

### Inference Optimizations

After testing the performance of our sub agents and combined pipeline, we chose not to implement any inference optimization techniques such as quantization, distillation, or multi-adapter swapping. This was mainly due to the fact that our models were already performing well and we did not want to risk losing any performance by implementing these techniques. However, this is something we will consider in the future as we continue to improve our assistant.

<br>
<!-- Reference to next section -->
<div style="text-align: right; font-size: 14px;">
  <a href="6_tools.html">Next Section: Tools and Frameworks</a>
</div>

---

### References:

${^1}$ [LangChain](https://python.langchain.com/docs/)

${^2}$ [Hugging Face](https://huggingface.co/)

${^3}$ [Bedrock](https://www.bedrock.aws/)

${^4}$ [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/)