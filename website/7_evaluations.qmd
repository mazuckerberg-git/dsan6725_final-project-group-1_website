---
title: "Evaluation of Effectiveness"
format:
  html:
    toc: true
    embed-resources: true
---

## Ragas Evaluations

To evaluate the performance of our Stock Market LLM Assistant, we used the `ragas` package, which provides specialized tools for assessing Large Language Model applications. The first step in the evaluation process was the creation of a test dataset focused specifically on news data queries. Since users generally query one ticker at a time, each dataset was constructed for individual tickers.

We leveraged Bedrock Embeddings through the langchain package to generate the embeddings required for retrieval. From the `ragas` package, we utilized the LLM Wrapper, the LangChain Embeddings Wrapper, and the Test Set Generator components. One limitation of this method was the long runtime required to generate the test datasets. However, an important advantage was the ability to define a persona, in this case a financial analyst, to provide a more sophisticated and professional interpretation of company-related news articles.

Once the test datasets were ready, we used the same RAG chain agent setup, incorporating a vectorstore retriever, to generate responses to the test prompts. We then evaluated these responses using three key metrics provided by ragas: *LLMContextRecall()*, *Faithfulness()*, and *FactualCorrectness() (mode=f1)*. The average metric results for each evaluated ticker were as follows:

| Ticker | Context Recall | Faithfulness | Factual Correctness (f1) |
|--------|----------------|--------------|--------------------------|
|  AAPL  | 0.8735         | 0.8438       | 0.4465                   |
|  ABNB  | 0.7778         | 0.6918       | 0.3500                   |
|  AMZN  | 0.7487         | 0.7532       | 0.3531                   |
|  GOOGL | 0.8265         | 0.7695       | 0.4817                   |
|  NFLX  | 0.7037         | 0.8117       | 0.4907                   |
|  NVDA  | 0.6710         | 0.8107       | 0.3944                   |
|  TSLA  | 0.8093         | 0.7673       | 0.3983                   |

From the results, we observe that the assistant demonstrates strong performance in terms of Context Recall and Faithfulness, indicating that the model effectively retrieves and maintains relevant information from the news articles and delivers consistent answers. On the other hand, the Factual Correctness scores are somewhat lower across all tickers. This outcome is expected given that the initial test set inputs are generated by an LLM, meaning there is an inherent margin of error in the prompts themselves. 

To futher evaluate the performance of our Stock Market LLM Assistant, we obatained user feedback through testing.

## User Feedback

In addition to the evaluations performed on the RAG Chain agent for news data, we also conducted qualitative assessments to evaluate the assistant’s outputs on price data queries and on the Data Engineering agent’s `code generation` and `documentation` tools. For price data, we enhanced the prompts sent to the RAG agent to explicitly instruct it to retrieve information only from the price vectorstore, minimizing the risk of hallucinations. As a result, the assistant consistently produced perfect answers for user queries related to stock prices. Some examples of these successful interactions are provided below:

```{=html}
<div style="display: flex; justify-content: space-around; align-items: center;">
  <div style="text-align: center;">
    <img src="../images/stock_price_query.png" alt="AMZN Price Queries" style="width: 400px; height: auto;">
    <p style="font-size: 10px">AMZN Price Queries</p>
  </div>
</div>
```

For the Data Engineering agent, we tested a variety of queries that included:

  * Generating statistical analysis.
  * Generating time series plots.
  * Generating Smoothing Moving Average plots.
  * Generating financial reports.
  * Generating time series analysis including additional visualizations such as ACF /  PCF, lab plots, etc.

These tasks initially required further prompt engineering to enhance and fine-tune the outputs, but ultimately the agent was able to produce exactly the expected results. The plots and analyses shown below demonstrate that the Data Engineering agent successfully executed the requested tasks using its `code generation` and `code execution` tools.

```{=html}
<div style="display: flex; justify-content: space-around; align-items: center;">
  <div style="text-align: center;">
    <img src="../images/nvidia_time_series.png" alt="NVDA Time Series Plot" style="width: 450px; height: auto;">
    <p style="font-size: 10px;">NVDA Time Series Plot</p>
  </div>
  <div style="text-align: center;">
    <img src="../images/amzn_moving_averages.png" alt="AMZN Time Series Plot" style="width: 420px; height: auto;">
    <p style="font-size: 10px">AMZN Time Series Plot</p>
  </div>
</div>
```

Based on these qualitative assessments, we can conclude that the assistant’s overall performance is aligned with our project expectations, effectively delivering meaningful and well-structured outputs across all components of the application.


<br>
<!-- Reference to next section -->
<div style="text-align: right; font-size: 14px;">
  <a href="8_AI_considerations.html">Next Section: Responsible AI Considerations</a>
</div>


---

### References:

${^1}$ [Ragas](https://docs.ragas.io/en/stable/)