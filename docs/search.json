[
  {
    "objectID": "8_AI_considerations.html",
    "href": "8_AI_considerations.html",
    "title": "Responsible AI Considerations",
    "section": "",
    "text": "Bias and fairness checks conducted.\nAny measures taken to reduce hallucinations and ensure factual consistency.\nPrivacy considerations (e.g., PII removal, data anonymization).\nModel safety measures and ethical implications.\n\n \n\nNext Section: Findings and Insights\n\n\n\nReferences:\n\\({^1}\\)"
  },
  {
    "objectID": "9_findings.html",
    "href": "9_findings.html",
    "title": "Findings and Insights",
    "section": "",
    "text": "Key takeaways from the project.\nUnexpected challenges and how they were addressed.\nPerformance trade-offs and scalability considerations. –&gt; In Conclusion\n\n \n\nNext Section: Conclusion and Future Work\n\n\n\nReferences:\n\\({^1}\\)"
  },
  {
    "objectID": "7_evaluations.html",
    "href": "7_evaluations.html",
    "title": "Evaluation of Effectiveness",
    "section": "",
    "text": "To evaluate the performance of our Stock Market LLM Assistant, we used the ragas package, which provides specialized tools for assessing Large Language Model applications. The first step in the evaluation process was the creation of a test dataset focused specifically on news data queries. Since users generally query one ticker at a time, each dataset was constructed for individual tickers.\nWe leveraged Bedrock Embeddings through the langchain package to generate the embeddings required for retrieval. From the ragas package, we utilized the LLM Wrapper, the LangChain Embeddings Wrapper, and the Test Set Generator components. One limitation of this method was the long runtime required to generate the test datasets. However, an important advantage was the ability to define a persona, in this case a financial analyst, to provide a more sophisticated and professional interpretation of company-related news articles.\nOnce the test datasets were ready, we used the same RAG chain agent setup, incorporating a vectorstore retriever, to generate responses to the test prompts. We then evaluated these responses using three key metrics provided by ragas: LLMContextRecall(), Faithfulness(), and FactualCorrectness() (mode=f1). The average metric results for each evaluated ticker were as follows:\n\n\n\nTicker\nContext Recall\nFaithfulness\nFactual Correctness (f1)\n\n\n\n\nAAPL\n0.8735\n0.8438\n0.4465\n\n\nABNB\n0.7778\n0.6918\n0.3500\n\n\nAMZN\n0.7487\n0.7532\n0.3531\n\n\nGOOGL\n0.8265\n0.7695\n0.4817\n\n\nNFLX\n0.7037\n0.8117\n0.4907\n\n\nNVDA\n0.6710\n0.8107\n0.3944\n\n\nTSLA\n0.8093\n0.7673\n0.3983\n\n\n\nFrom the results, we observe that the assistant demonstrates strong performance in terms of Context Recall and Faithfulness, indicating that the model effectively retrieves and maintains relevant information from the news articles and delivers consistent answers. On the other hand, the Factual Correctness scores are somewhat lower across all tickers. This outcome is expected given that the initial test set inputs are generated by an LLM, meaning there is an inherent margin of error in the prompts themselves.\nTo futher evaluate the performance of our Stock Market LLM Assistant, we obatained user feedback through testing."
  },
  {
    "objectID": "7_evaluations.html#ragas-evaluations",
    "href": "7_evaluations.html#ragas-evaluations",
    "title": "Evaluation of Effectiveness",
    "section": "",
    "text": "To evaluate the performance of our Stock Market LLM Assistant, we used the ragas package, which provides specialized tools for assessing Large Language Model applications. The first step in the evaluation process was the creation of a test dataset focused specifically on news data queries. Since users generally query one ticker at a time, each dataset was constructed for individual tickers.\nWe leveraged Bedrock Embeddings through the langchain package to generate the embeddings required for retrieval. From the ragas package, we utilized the LLM Wrapper, the LangChain Embeddings Wrapper, and the Test Set Generator components. One limitation of this method was the long runtime required to generate the test datasets. However, an important advantage was the ability to define a persona, in this case a financial analyst, to provide a more sophisticated and professional interpretation of company-related news articles.\nOnce the test datasets were ready, we used the same RAG chain agent setup, incorporating a vectorstore retriever, to generate responses to the test prompts. We then evaluated these responses using three key metrics provided by ragas: LLMContextRecall(), Faithfulness(), and FactualCorrectness() (mode=f1). The average metric results for each evaluated ticker were as follows:\n\n\n\nTicker\nContext Recall\nFaithfulness\nFactual Correctness (f1)\n\n\n\n\nAAPL\n0.8735\n0.8438\n0.4465\n\n\nABNB\n0.7778\n0.6918\n0.3500\n\n\nAMZN\n0.7487\n0.7532\n0.3531\n\n\nGOOGL\n0.8265\n0.7695\n0.4817\n\n\nNFLX\n0.7037\n0.8117\n0.4907\n\n\nNVDA\n0.6710\n0.8107\n0.3944\n\n\nTSLA\n0.8093\n0.7673\n0.3983\n\n\n\nFrom the results, we observe that the assistant demonstrates strong performance in terms of Context Recall and Faithfulness, indicating that the model effectively retrieves and maintains relevant information from the news articles and delivers consistent answers. On the other hand, the Factual Correctness scores are somewhat lower across all tickers. This outcome is expected given that the initial test set inputs are generated by an LLM, meaning there is an inherent margin of error in the prompts themselves.\nTo futher evaluate the performance of our Stock Market LLM Assistant, we obatained user feedback through testing."
  },
  {
    "objectID": "7_evaluations.html#user-feedback",
    "href": "7_evaluations.html#user-feedback",
    "title": "Evaluation of Effectiveness",
    "section": "User Feedback",
    "text": "User Feedback\nIn addition to the evaluations performed on the RAG Chain agent for news data, we also conducted qualitative assessments to evaluate the assistant’s outputs on price data queries and on the Data Engineering agent’s code generation and documentation tools. For price data, we enhanced the prompts sent to the RAG agent to explicitly instruct it to retrieve information only from the price vectorstore, minimizing the risk of hallucinations. As a result, the assistant consistently produced perfect answers for user queries related to stock prices. Some examples of these successful interactions are provided below:\n\n  \n    \n    AMZN Price Queries\n  \n\nFor the Data Engineering agent, we tested a variety of queries that included:\n\nGenerating statistical analysis.\nGenerating time series plots.\nGenerating Smoothing Moving Average plots.\nGenerating financial reports.\nGenerating time series analysis including additional visualizations such as ACF / PCF, lab plots, etc.\n\nThese tasks initially required further prompt engineering to enhance and fine-tune the outputs, but ultimately the agent was able to produce exactly the expected results. The plots and analyses shown below demonstrate that the Data Engineering agent successfully executed the requested tasks using its code generation and code execution tools.\n\n  \n    \n    NVDA Time Series Plot\n  \n  \n    \n    AMZN Time Series Plot\n  \n\nBased on these qualitative assessments, we can conclude that the assistant’s overall performance is aligned with our project expectations, effectively delivering meaningful and well-structured outputs across all components of the application.\n \n\nNext Section: Responsible AI Considerations\n\n\n\nReferences:\n\\({^1}\\) Ragas"
  },
  {
    "objectID": "2_data.html",
    "href": "2_data.html",
    "title": "Data Sources",
    "section": "",
    "text": "For our Stock Market LLM Assistant project we selected data sources that directly align with the requirements of financial analysis and stock market insight generation. As the project focuses on stock market evaluation, our primary data source is stock price information from Yahoo Finance. This data is crucial for understanding price fluctuations, identifying trends, comparing performance of different stocks and enabling robust statistical analysis.\nTo complement price data and provide users with deeper insights, we also included news data from the recent news section associated with each ticker on Yahoo Finance. This allows the Assistant justify its answers and produce more informed, complete financial evaluations.\nWe gather ticker prices by scraping Yahoo Finance using the requests package in Python to retrieve detailed information such as Trading Date, Open Price, High Price, Low Price, Close Price, Company Name, among others.\nFor the news data, we use the Finnhub Stock API, as well-structured, highly regarded financial API built around REST principles. Using an API key, we extract company news within a specified date range and collect information such as Headline, Source, Summary, and URL."
  },
  {
    "objectID": "2_data.html#data-preparation",
    "href": "2_data.html#data-preparation",
    "title": "Data Sources",
    "section": "Data Preparation",
    "text": "Data Preparation\nSince the price data is collected through web scraping, it was essential to perform several cleaning and transformation steps to ensure data quality and consistency. Some of the primary preprocessing tasks included converting fields from integer formats to proper datetime objects, and ensuring that all price-related fields were stored as numeric types. To facilitate better filtering and organization, we also augmented the data by adding key metadata, such as the ticker name, the month and year extracted from the trading date, and a source label indicating whether the entry corresponded to price or news data.\nFor the news data obtained through the Finnhub API, the information was already structured and relatively clean. The main transformation involved converting the timestamp field from an integer format into a datetime object.\nThese preprocessing and augmentation steps are applied consistently across all tickers, ensuring a uniform structure. Finally, all processed data is saved both locally and in Amazon S3 buckets in JSON format, optimized for future loading into FAISS vector stores for efficient retrieval.\n \n\nNext Section: Retrieval-Augmented Generation (RAG)\n\n\n\nReferences:\n\\({^1}\\) Finnhub-API\n\\({^2}\\) Yahoo Finance"
  },
  {
    "objectID": "10_conclusion.html",
    "href": "10_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of the project and its impact."
  },
  {
    "objectID": "10_conclusion.html#limitations",
    "href": "10_conclusion.html#limitations",
    "title": "Conclusion",
    "section": "Limitations",
    "text": "Limitations\n\nLimitations of the current implementation."
  },
  {
    "objectID": "10_conclusion.html#future-improvememnents",
    "href": "10_conclusion.html#future-improvememnents",
    "title": "Conclusion",
    "section": "Future Improvememnents",
    "text": "Future Improvememnents\n\nPotential future improvements and extensions."
  },
  {
    "objectID": "6_tools.html",
    "href": "6_tools.html",
    "title": "Tools",
    "section": "",
    "text": "Programming languages and frameworks used (e.g., Python, PyTorch, TensorFlow, Hugging Face, OpenAI API).\nCloud platforms and any relevant services (e.g., Amazon Bedrock, Amazon SageMaker, Vector DBs like FAISS), open-source (e.g. LangChain).\nVersion control, CI/CD, or deployment considerations.\nMention Langchain for all agents\nMention Bedrock for invoking the models\nFAISS mentioned in models?\nVersion control was done using Git"
  },
  {
    "objectID": "6_tools.html#frameworks",
    "href": "6_tools.html#frameworks",
    "title": "Tools",
    "section": "Frameworks",
    "text": "Frameworks\nMention streamlit interface with in chat loggers\n \n\nNext Section: “Evaluation of Effectiveness\n\n\n\nReferences:\n\\({^1}\\) LangChain\n\\({^2}\\) Bedrock\n\\({^3}\\) Streamlit\n\\({^4}\\) [Github] (https://docs.github.com/en/get-started/using-git/about-git)"
  },
  {
    "objectID": "3_rag.html",
    "href": "3_rag.html",
    "title": "Retrieval-Augmented Generation (RAG)",
    "section": "",
    "text": "Within our multi-agent pipeline, we have implemented a Retrieval-Augmented Generation (RAG) system to act as the question-answering portion of the application. The RAG system is designed to read the user’s query, retrieve the sources needed in the vectorstore to answer the question, and consolidate the sources to provide an accurate answer. This is especially important for stock market data, as the information is constantly changing, includes exact values, and is often time-sensitive. With a RAG we’re able to avoid any type of hallucination and provide the most accurate and up-to-date information to the user.\nThe RAG system is built using the LangChain framework, which allows us to easily integrate various components such as the vectorstore, retriever, and LLM. The vectorstore was created using FAISS, a popular library for efficient similarity search and clustering of dense vectors. The embeddings used to create the vectorstore were generated from Hugging Face’s intfloat/e5-small-v2 model, which is lightweight and efficient with both semantic understanding and numeric retrieval capabilities. Our retriever is uses the maximum marginal relevance (MMR) algorithm to ensure that the retrieved documents are both relevant and diverse, which is crucial for providing comprehensive answers to user queries. Additionally, we have set k in the retriever to 30 since we found the RAG to suffer in retrieving the correct documents when k was lower, mainly due to the fact the RAG struggles with filtering data by specific dates and names. Lastly, the LLM used for answer generation is Claude 3.5 Sonnet from Anthropic, which is a state-of-the-art model known for its reasoning capabilities and ability to generate coherent and contextually relevant responses with being too wordy.\nWhen it came to creating the structure of the RAG, we also need to take into consideration the chat history and how to best utilize it. We decided that rather than creating a RAG chain, we would seperate it by question answer chain and document retrieval. We first combine the user query and the chat history into a single string, which is then passed to the retriever to find the most relevant documents. The retrieved documents are then passed to the question answer chain, (made up of the llm, system prompt, and documents) to generate a response. This approach allows us to maintain the context of the conversation while ensuring that the RAG system can effectively retrieve and utilize the most relevant information.\nThe RAG system is able to provide a cohesive answer, either directly based on the retrived documents or a statement that not enough data was found, back to the supervisor agent to be sent to the user. The system is build such that the supervisor agent simple transfers the RAG’s output to the user instead of augmenting any additonal details to avoid any type of hallucination.\n \n\nNext Section: AI Agents\n\n\n\nReferences:\n\\({^1}\\) LangChain\n\\({^2}\\) Hugging Face\n\\({^3}\\) FAISS"
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Our Stock Market LLM Assistant will be designed to provide insights on stock market data using a multi-agent system. The application leverages data engineering and RAG (Retrieval-Augmented Generation) agents to automate data collection, processing, and query handling. The data agent gathers stock market data via APIs, and cleans and stores it using FAISS. The engineering agent generates visualizations, executes code, and documents processes. The RAG agent answers user queries, analyzes stock trends, compares tickers, and generates customized datasets and visualizations. Our fully-functioning generative AI model will be contained in a user-friendly application which offers an interactive and intuitive experience for users seeking stock market insights.\nOur goal for this assistant is to create an application that is accessible to a wide range of users; from novice investors to experienced traders. The streamlit interface acts like a chat bot that allows users to ask questions about stock market data and receive real-time insights. Automated report generation, especially in finance, is a powerful and needed tool in the corporate world right now and we believe this project to serve as a stepping stone into more sophisticated iterations in the future for others to build upon."
  },
  {
    "objectID": "1_introduction.html#points-of-analysis",
    "href": "1_introduction.html#points-of-analysis",
    "title": "Introduction",
    "section": "Points of Analysis",
    "text": "Points of Analysis\n\nData Science Question\n\nHow can we build a Stock Market LLM Assistant leveraging multi-agent systems to deliver users with an efficient, intuitive interface for comprehensive financial and technical stock market analysis?\n\n\n\nProject Questions:\n\nWhat sources of data are the most relevant to provide users with accurate and timely stock market insights?\n\n\nCan we create a multi-agent system that automates data collection, processing, and query handling for stock market analysis?\n\n\nWhat models and techniques can we implement for RAG to optimize query-to-context retrieval and response generation?\n\n\nCan we implement tools that allow users to generate customized financial reports and visualizations?\n\n\nShould we consider AI ethics and bias in our model to ensure uninformed decision-making?\n\n\nWhat are future improvements we can make to enhance the performance and usability of the Stock Market LLM Assistant?\n\n \n\nNext Section: Data Sources and Preparation\n\n\n\n\nReferences:\n\\({^1}\\) Gamage, P. (2026). Applied Time Series for Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Speeding up Stock Market Insights with Generative AI",
    "section": "",
    "text": "M.S. in Data Science and Analytics\n    \n\n    Applied Generative AI for Developers\n    Final Project\n    Speeding up Stock Market Insights with Generative AI\n    A Multi-Agent System for Stock Market Analysis\n    By: Shriya Chinthak and Maria Agustina Zuckerberg"
  },
  {
    "objectID": "4_agents.html",
    "href": "4_agents.html",
    "title": "AI Agents",
    "section": "",
    "text": "Architecture of AI agents used.\nFrameworks used (e.g., LangChain, LlamaIndex, Haystack).\nTypes of agents (e.g., tool-using agents, autonomous agents, planning agents).\nHow agents interact with external tools and APIs.\nExplain overall structure, graph and in memory"
  },
  {
    "objectID": "4_agents.html#supervisor-agent",
    "href": "4_agents.html#supervisor-agent",
    "title": "AI Agents",
    "section": "Supervisor Agent",
    "text": "Supervisor Agent"
  },
  {
    "objectID": "4_agents.html#data-gathering-agent",
    "href": "4_agents.html#data-gathering-agent",
    "title": "AI Agents",
    "section": "Data Gathering Agent",
    "text": "Data Gathering Agent\nThe Data Gathering Agent is an autonomous agent designed to collect both stock price and news data based on the tickers specified in the user’s query. To achieve this, the agent leverages two tools: gather_data() and data_to_vectorstore(). Regardless of the user’s request type, this agent is always invoked as a first step following specific instructions. First, it identifies the tickers mentioned in the query and compiles them into a list to be passed into the gather_data() tool. Then, it gathers the most up-to-date price and news data for the defined tickers. Finally, if new data has been successfully retrieved, the agent either creates a new vectorstore or updates an existing one, ensuring that the information is ready to be used by the next agents in the process: the RAG Agent and the Data Engineering Agent."
  },
  {
    "objectID": "4_agents.html#rag-agent",
    "href": "4_agents.html#rag-agent",
    "title": "AI Agents",
    "section": "RAG Agent",
    "text": "RAG Agent"
  },
  {
    "objectID": "4_agents.html#data-engineering-agent",
    "href": "4_agents.html#data-engineering-agent",
    "title": "AI Agents",
    "section": "Data Engineering Agent",
    "text": "Data Engineering Agent\n \n\nNext Section: Models and Technologies Used\n\n\n\nReferences:\n\\({^1}\\)"
  },
  {
    "objectID": "5_model.html",
    "href": "5_model.html",
    "title": "Models",
    "section": "",
    "text": "Discussion on the models utilized (LLMs, embedding models, classifiers, etc.).\nComparison of different models and justification for final choices.\nAny inference optimizations applied (e.g., quantization, distillation, multi-adapter swapping).\nMention FAISS embeddings for tabular data\nMention Claude for code and answer generation"
  },
  {
    "objectID": "5_model.html#technologies-used",
    "href": "5_model.html#technologies-used",
    "title": "Models",
    "section": "Technologies Used",
    "text": "Technologies Used\n \n\nNext Section: Tools and Frameworks\n\n\n\nReferences:\n\\({^1}\\) LangChain\n\\({^2}\\) Hugging Face\n\\({^3}\\) Bedrock\n\\({^4}\\) FAISS"
  }
]