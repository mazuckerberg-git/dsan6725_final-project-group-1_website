[
  {
    "objectID": "8_AI_considerations.html",
    "href": "8_AI_considerations.html",
    "title": "Responsible AI Considerations",
    "section": "",
    "text": "To ensure that our Stock Market LLM Assistant is fair and unbiased, and most importantly, avoided hallucinations, we mainly focused on the following aspects:\n\nLLM Temperature: We set the temperature of our LLM to 0.2, which is a low value that helps to reduce the randomness of the model’s output. This means that the model is less likely to generate unexpected or irrelevant responses, which can help to reduce bias and improve the overall quality of the generated text. This is particularly important in the context of stock market analysis, where accuracy to the data extracted and reliability are crucial.\nRAG Hallucination Reduction: During the first iteration of our RAG agent, we noticed the model was generating hallucinations, which are false or misleading information. To address this issue, we first appended filterable data to the “metadata” portion of our json structure prior to storing it in the vectorstore. This metadata includes the datetime of the stock data, the name of the ticker, and the source if it was price data or news data. Additionally, we also changed our embedding model to intfloat/e5-small-v2 to improve the RAG agent’s ability to retrieve relevant data. Lastly, we increased the retriever k value to 30 to ensure that the model retrieves the most relevant data from the vectorstore to answer the user’s query. These improvements resulted in an agent that provides more accurate information and reduces the risk of hallucinations.\nUsing Publicly Available Data: We used publicly available data from Yahoo Finance and Finnhub as our knowledge base. This data is widely used in the finance industry and is considered to be reliable and accurate. By using this data, we can ensure that our model is not biased towards any specific source or dataset.\nManual Review of Outputs: We conducted a manual review of the outputs generated by our model to ensure that they are accurate and unbiased. This involved checking the outputs against the original data and ensuring that they are consistent with the user’s query.\nNotice to Users: We included a notice to users that the model is only for educational purposes and statistical analysis. We wanted to ensure that users understand the limitations of the model and that they should not rely solely on its outputs for making investment decisions. The notice we provide is listed below:\n\n\nNotice: This report provides code generated, explanations of key decisions made by the LLM system, and the resulting outputs and insights. All insights are derived from statistical and time series analysis. Any financial decisions made based on this information are solely at the discretion of the user. The assistant does not constitute financial advice and should not be relied on for investment decisions.\n\nThus, we have taken several steps to ensure that our Stock Market LLM Assistant is fair and unbiased. By focusing on the temperature of the LLM, reducing hallucinations in the RAG agent, using publicly available data, conducting manual reviews of outputs, and providing a notice to users, we can ensure that our model is reliable and accurate while acknowledging its limitations.\n \n\nNext Section: Findings and Insights"
  },
  {
    "objectID": "8_AI_considerations.html#bias-and-fairness",
    "href": "8_AI_considerations.html#bias-and-fairness",
    "title": "Responsible AI Considerations",
    "section": "",
    "text": "To ensure that our Stock Market LLM Assistant is fair and unbiased, and most importantly, avoided hallucinations, we mainly focused on the following aspects:\n\nLLM Temperature: We set the temperature of our LLM to 0.2, which is a low value that helps to reduce the randomness of the model’s output. This means that the model is less likely to generate unexpected or irrelevant responses, which can help to reduce bias and improve the overall quality of the generated text. This is particularly important in the context of stock market analysis, where accuracy to the data extracted and reliability are crucial.\nRAG Hallucination Reduction: During the first iteration of our RAG agent, we noticed the model was generating hallucinations, which are false or misleading information. To address this issue, we first appended filterable data to the “metadata” portion of our json structure prior to storing it in the vectorstore. This metadata includes the datetime of the stock data, the name of the ticker, and the source if it was price data or news data. Additionally, we also changed our embedding model to intfloat/e5-small-v2 to improve the RAG agent’s ability to retrieve relevant data. Lastly, we increased the retriever k value to 30 to ensure that the model retrieves the most relevant data from the vectorstore to answer the user’s query. These improvements resulted in an agent that provides more accurate information and reduces the risk of hallucinations.\nUsing Publicly Available Data: We used publicly available data from Yahoo Finance and Finnhub as our knowledge base. This data is widely used in the finance industry and is considered to be reliable and accurate. By using this data, we can ensure that our model is not biased towards any specific source or dataset.\nManual Review of Outputs: We conducted a manual review of the outputs generated by our model to ensure that they are accurate and unbiased. This involved checking the outputs against the original data and ensuring that they are consistent with the user’s query.\nNotice to Users: We included a notice to users that the model is only for educational purposes and statistical analysis. We wanted to ensure that users understand the limitations of the model and that they should not rely solely on its outputs for making investment decisions. The notice we provide is listed below:\n\n\nNotice: This report provides code generated, explanations of key decisions made by the LLM system, and the resulting outputs and insights. All insights are derived from statistical and time series analysis. Any financial decisions made based on this information are solely at the discretion of the user. The assistant does not constitute financial advice and should not be relied on for investment decisions.\n\nThus, we have taken several steps to ensure that our Stock Market LLM Assistant is fair and unbiased. By focusing on the temperature of the LLM, reducing hallucinations in the RAG agent, using publicly available data, conducting manual reviews of outputs, and providing a notice to users, we can ensure that our model is reliable and accurate while acknowledging its limitations.\n \n\nNext Section: Findings and Insights"
  },
  {
    "objectID": "9_findings.html",
    "href": "9_findings.html",
    "title": "Findings and Insights",
    "section": "",
    "text": "Our Stock Market LLM Assistant successfully integrates a multi-agent system to automate data collection, processing, and query handling for stock market analysis and general insights.\nThe use of LangChain’s StateGraph package for the Supervisor Agent allowed for efficient management of the flow of information between the user and the sub-agents.\nWe were able to create a product that is user-friendly, accurate, and useful beyond the classroom."
  },
  {
    "objectID": "9_findings.html#key-takeaways",
    "href": "9_findings.html#key-takeaways",
    "title": "Findings and Insights",
    "section": "",
    "text": "Our Stock Market LLM Assistant successfully integrates a multi-agent system to automate data collection, processing, and query handling for stock market analysis and general insights.\nThe use of LangChain’s StateGraph package for the Supervisor Agent allowed for efficient management of the flow of information between the user and the sub-agents.\nWe were able to create a product that is user-friendly, accurate, and useful beyond the classroom."
  },
  {
    "objectID": "9_findings.html#unexpected-challenges",
    "href": "9_findings.html#unexpected-challenges",
    "title": "Findings and Insights",
    "section": "Unexpected Challenges",
    "text": "Unexpected Challenges\nWith such an elaborate system, we of course ran into some unexpected challenges. The most notable ones were:\n\nData Collection: We faced challenges when it came to collecting up-to-date data. We had to rework our original code to update the FAISS vectorstore index only if the data does not already exist in the S3 bucket (e.i. the user has not asked for that data yet). Otherwise, the vectorstore simply loads the existing up-to-date FAISS index file. Additionally, if we are collecting data, we also only collect dates of data that are not already in the vectorstore. This was a challenge because we had to ensure that the data was being collected correctly and efficiently, without duplicating data or causing errors in the system.\nRAG Hallucination: As we previously mentioned, the RAG agent had to be reworked to ensure that it was not hallucinating. By adding metadata to the json formatted data, changing the embedding model, rewriting the user prompt to include key details from the chat history, and increasing the number of documents retrieved, we were able to reduce the hallucination rate significantly.\nPrompt Engineering: We had to rewrite several of our prompts to ensure that the agent was executing tasks exactly as we wanted. This included changing the way the agents were generating code, the way it was executing code, and the way it was generating documentation, how the formatting of answers should be, where outputs will be located (file paths), and much more. This was both tedious and time consuming, but necessary to ensure that the system was working as intended.\nThrottling Errors in AWS Bedrock: By far, the most frustrating challenge of this project was facing throttling errors in AWS Bedrock. This error tended to be a catch all error for several issues in the pipeline, which made it incredibly difficult to debug. At times, we had to copy our code into chunks, rework slightly to test outside of the pipeline, and run it in a jupyter notebook to ensure the code was working as intended. Other times, we simply had to wait since AWS Bedrock would be down due to the amount of requests being made across the platform. This delayed progress some days and made it difficult to test the system as a whole.\n\n \n\nNext Section: Conclusion\n\n\n\nReferences:\n\\({^1}\\) AWS Berock Throttling Errors"
  },
  {
    "objectID": "7_evaluations.html",
    "href": "7_evaluations.html",
    "title": "Evaluation of Effectiveness",
    "section": "",
    "text": "To evaluate the performance of our Stock Market LLM Assistant, we used the ragas package, which provides specialized tools for assessing Large Language Model applications. The first step in the evaluation process was the creation of a test dataset focused specifically on news data queries. Since users generally query one ticker at a time, each dataset was constructed for individual tickers.\nWe leveraged Bedrock Embeddings through the langchain package to generate the embeddings required for retrieval. From the ragas package, we utilized the LLM Wrapper, the LangChain Embeddings Wrapper, and the Test Set Generator components. One limitation of this method was the long runtime required to generate the test datasets. However, an important advantage was the ability to define a persona, in this case a financial analyst, to provide a more sophisticated and professional interpretation of company-related news articles.\nOnce the test datasets were ready, we used the same RAG chain agent setup, incorporating a vectorstore retriever, to generate responses to the test prompts. We then evaluated these responses using three key metrics provided by ragas: LLMContextRecall(), Faithfulness(), and FactualCorrectness() (mode=f1). The average metric results for each evaluated ticker were as follows:\n\n\n\nTicker\nContext Recall\nFaithfulness\nFactual Correctness (f1)\n\n\n\n\nAAPL\n0.8735\n0.8438\n0.4465\n\n\nABNB\n0.7778\n0.6918\n0.3500\n\n\nAMZN\n0.7487\n0.7532\n0.3531\n\n\nGOOGL\n0.8265\n0.7695\n0.4817\n\n\nNFLX\n0.7037\n0.8117\n0.4907\n\n\nNVDA\n0.6710\n0.8107\n0.3944\n\n\nTSLA\n0.8093\n0.7673\n0.3983\n\n\n\nFrom the results, we observe that the assistant demonstrates strong performance in terms of Context Recall and Faithfulness, indicating that the model effectively retrieves and maintains relevant information from the news articles and delivers consistent answers. On the other hand, the Factual Correctness scores are somewhat lower across all tickers. This outcome is expected given that the initial test set inputs are generated by an LLM, meaning there is an inherent margin of error in the prompts themselves.\nTo further evaluate the performance of our Stock Market LLM Assistant, we obtained user feedback through testing."
  },
  {
    "objectID": "7_evaluations.html#ragas-evaluations",
    "href": "7_evaluations.html#ragas-evaluations",
    "title": "Evaluation of Effectiveness",
    "section": "",
    "text": "To evaluate the performance of our Stock Market LLM Assistant, we used the ragas package, which provides specialized tools for assessing Large Language Model applications. The first step in the evaluation process was the creation of a test dataset focused specifically on news data queries. Since users generally query one ticker at a time, each dataset was constructed for individual tickers.\nWe leveraged Bedrock Embeddings through the langchain package to generate the embeddings required for retrieval. From the ragas package, we utilized the LLM Wrapper, the LangChain Embeddings Wrapper, and the Test Set Generator components. One limitation of this method was the long runtime required to generate the test datasets. However, an important advantage was the ability to define a persona, in this case a financial analyst, to provide a more sophisticated and professional interpretation of company-related news articles.\nOnce the test datasets were ready, we used the same RAG chain agent setup, incorporating a vectorstore retriever, to generate responses to the test prompts. We then evaluated these responses using three key metrics provided by ragas: LLMContextRecall(), Faithfulness(), and FactualCorrectness() (mode=f1). The average metric results for each evaluated ticker were as follows:\n\n\n\nTicker\nContext Recall\nFaithfulness\nFactual Correctness (f1)\n\n\n\n\nAAPL\n0.8735\n0.8438\n0.4465\n\n\nABNB\n0.7778\n0.6918\n0.3500\n\n\nAMZN\n0.7487\n0.7532\n0.3531\n\n\nGOOGL\n0.8265\n0.7695\n0.4817\n\n\nNFLX\n0.7037\n0.8117\n0.4907\n\n\nNVDA\n0.6710\n0.8107\n0.3944\n\n\nTSLA\n0.8093\n0.7673\n0.3983\n\n\n\nFrom the results, we observe that the assistant demonstrates strong performance in terms of Context Recall and Faithfulness, indicating that the model effectively retrieves and maintains relevant information from the news articles and delivers consistent answers. On the other hand, the Factual Correctness scores are somewhat lower across all tickers. This outcome is expected given that the initial test set inputs are generated by an LLM, meaning there is an inherent margin of error in the prompts themselves.\nTo further evaluate the performance of our Stock Market LLM Assistant, we obtained user feedback through testing."
  },
  {
    "objectID": "7_evaluations.html#user-feedback",
    "href": "7_evaluations.html#user-feedback",
    "title": "Evaluation of Effectiveness",
    "section": "User Feedback",
    "text": "User Feedback\nIn addition to the evaluations performed on the RAG Chain agent for news data, we also conducted qualitative assessments to evaluate the assistant’s outputs on price data queries and on the Data Engineering agent’s code generation and documentation tools. For price data, we enhanced the prompts sent to the RAG agent to explicitly instruct it to retrieve information only from the price vectorstore, minimizing the risk of hallucinations. As a result, the assistant consistently produced perfect answers for user queries related to stock prices. Some examples of these successful interactions are provided below:\n\n \n   \n   AMZN Price Queries\n \n\nFor the Data Engineering agent, we tested a variety of queries that included:\n\nGenerating statistical analysis.\nGenerating time series plots.\nGenerating Smoothing Moving Average plots.\nGenerating financial reports.\nGenerating time series analysis including additional visualizations such as ACF / PCF, lab plots, etc.\n\nThese tasks initially required further prompt engineering to enhance and fine-tune the outputs, but ultimately the agent was able to produce exactly the expected results. The plots and analyses shown below demonstrate that the Data Engineering agent successfully executed the requested tasks using its code generation and code execution tools.\n\n \n   \n   NVDA Time Series Plot\n \n \n   \n   AMZN Time Series Plot\n \n\nBased on these qualitative assessments, we can conclude that the assistant’s overall performance is aligned with our project expectations, effectively delivering meaningful and well-structured outputs across all components of the application.\n \n\nNext Section: Responsible AI Considerations\n\n\n\nReferences:\n\\({^1}\\) Ragas"
  },
  {
    "objectID": "2_data.html",
    "href": "2_data.html",
    "title": "Data Sources",
    "section": "",
    "text": "For our Stock Market LLM Assistant project, we selected data sources that directly align with the requirements of financial analysis and stock market insight generation. As the project focuses on stock market evaluation, our primary data source is stock price information from Yahoo Finance. This data is crucial for understanding price fluctuations, identifying trends, comparing performance of different stocks and enabling robust statistical analysis.\nTo complement price data and provide users with deeper insights, we also included news data from the recent news section associated with each ticker on Yahoo Finance. This allows the Assistant to justify its answers and produce more informed and complete financial evaluations.\nWe gather ticker prices by scraping Yahoo Finance using the requests package in Python to retrieve detailed information such as Trading Date, Open Price, High Price, Low Price, Close Price, Company Name, among others.\nFor the news data, we use the Finnhub Stock API, a well-structured, highly regarded financial API built around REST principles. Using an API key, we extract company news within a specified date range and collect information such as Headline, Source, Summary, and URL."
  },
  {
    "objectID": "2_data.html#data-preparation",
    "href": "2_data.html#data-preparation",
    "title": "Data Sources",
    "section": "Data Preparation",
    "text": "Data Preparation\nSince the price data is collected through web scraping, it was essential to perform several cleaning and transformation steps to ensure data quality and consistency. Some of the primary preprocessing tasks included converting fields from integer formats to proper datetime objects, and ensuring that all price-related fields were stored as numeric types. To facilitate better filtering and organization, we also augmented the data by adding key metadata, such as the ticker name, the month and year extracted from the trading date, and a source label indicating whether the entry corresponded to price or news data.\nFor the news data obtained through the Finnhub API, the information was already structured and relatively clean. The main transformation involved converting the timestamp field from an integer format into a datetime object.\nThese preprocessing and augmentation steps are applied consistently across all tickers, ensuring a uniform structure. Finally, all processed data is saved both locally and in Amazon S3 buckets in JSON format, optimized for future loading into FAISS vector stores for efficient retrieval.\n \n\nNext Section: Retrieval-Augmented Generation\n\n\n\nReferences:\n\\({^1}\\) Finnhub-API\n\\({^2}\\) Yahoo Finance"
  },
  {
    "objectID": "10_conclusion.html",
    "href": "10_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "As we have described throughout this report, our Stock Market LLM Assistant is a multi-agent system that automates data collection, processing, and query handling for stock market analysis and insights. The system is designed to be user-friendly, accurate, and efficient, providing users with real-time insights into stock market data. We were able to utilize a variety of AWS services, python packages, and LLMs to create a product that is not only functional but also scalable and flexible.\nWe hope this project serves as an example of what generative AI can do in the financial sector and how it can be used to automate time-consuming and jargon-heavy tasks. This could hopefully be a useful tool to learn more about the stock market and make informed decisions, even as a novice in the field."
  },
  {
    "objectID": "10_conclusion.html#limitations",
    "href": "10_conclusion.html#limitations",
    "title": "Conclusion",
    "section": "Limitations",
    "text": "Limitations\nIn our current implementation, there are some limitations that we have encountered. These include:\n\nData Collection: The data collection process is dependent on the APIs we are using. We are only able to collect stock price data for the past two years from the current date due to the free API we are using. For news data, we are only able to retrieve articles from the past few weeks. This limits the amount of data we can collect and analyze, which may impact the questions that can be answered by the system.\nRAG Limitations: The RAG agent is still prone to being unable to answer certain questions based on the retrieval data. Because the agent is not explicitly filtering the documents when retrieving them, but rather, conducting a similarity search, it is possible that the agent will retrieve documents that are not relevant to the question being asked. While we have safeguards in place to ensure that the agent is not hallucinating, the agent will respond by saving it doesn’t have enough information even if it is stored in the vectorstore.\nLLM Limitations: The LLMs we are using are not perfect and may not always provide accurate or relevant answers to user queries. This is especially true for some of the more complex analysis that the user may request."
  },
  {
    "objectID": "10_conclusion.html#future-improvements",
    "href": "10_conclusion.html#future-improvements",
    "title": "Conclusion",
    "section": "Future Improvements",
    "text": "Future Improvements\nIf we were to continue this project, we would consider the following improvements:\n\nData Collection: We would consider using a paid API to collect more comprehensive data for stock prices and news articles. This would allow us to provide more accurate and timely insights to users as well as broaden the time period of data we can collect.\nBusiness and Financial Analysis: We would consider adding more advanced business and financial analysis tools to the system that are commonly asked for in the corporate financial world.\nFaster Responses: We would consider implementing more inference optimization techniques to improve the response time of the system since the current system is slow to respond for certain queries, especially with engineering tasks.\n\n \n\nNext Section: Video Demo"
  },
  {
    "objectID": "6_tools.html",
    "href": "6_tools.html",
    "title": "Tools",
    "section": "",
    "text": "The primary programming language used in our project is Python, which is widely used in the data science and machine learning communities. We utilized several libraries and frameworks to build our application, but the most notable ones are:\n\nLangChain: This is the main framework we used to build our multi-agent system. It provides a modular and flexible architecture for creating and managing agents, as well as tools for data retrieval and processing. LangChain’s documentation provides detailed information on how to use its various components.\nStreamlit: This is a Python library that allows us to create interactive web applications. We used Streamlit to build the user interface for our application, which acts as a chat interface for users to interact with our agents. The Streamlit documentation provides information on how to use its features and components."
  },
  {
    "objectID": "6_tools.html#programming-languages-and-frameworks",
    "href": "6_tools.html#programming-languages-and-frameworks",
    "title": "Tools",
    "section": "",
    "text": "The primary programming language used in our project is Python, which is widely used in the data science and machine learning communities. We utilized several libraries and frameworks to build our application, but the most notable ones are:\n\nLangChain: This is the main framework we used to build our multi-agent system. It provides a modular and flexible architecture for creating and managing agents, as well as tools for data retrieval and processing. LangChain’s documentation provides detailed information on how to use its various components.\nStreamlit: This is a Python library that allows us to create interactive web applications. We used Streamlit to build the user interface for our application, which acts as a chat interface for users to interact with our agents. The Streamlit documentation provides information on how to use its features and components."
  },
  {
    "objectID": "6_tools.html#cloud-platforms",
    "href": "6_tools.html#cloud-platforms",
    "title": "Tools",
    "section": "Cloud Platforms",
    "text": "Cloud Platforms\nUsing the skills we obtained in this course, we used AWS as our cloud platform to deploy our application. Additionally, we used Amazon Bedrock to access the LLMs and Amazon SageMaker for model training and deployment.\nFor the structure and creation of our agents, we used LangChain and its various modules as the framework for our multi-agent system. To create and store our data that the agents access, we used FAISS (Facebook AI Similarity Search) as our vector database. This allows us to store and retrieve data efficiently, as well as append new data to the vectorstore if requested by the user.\nLastly, to create our user interface, we used Streamlit, a Python library that allows us to create interactive web applications. Our streamlit app acts as a chat interface, allowing users to interact with our agents and receive real-time insights on stock market data.\n\nData Gathering Tools\n\n\nRAG Tools\nAs mentioned previously, we used LangChain to create our RAG agent. This agent is responsible for retrieving relevant data from our vector database and generating responses to user queries. Again, since we are using a rag chain to retrieve and answer user queries, we do not have any tools defined for this agent.\n\n\nData Engineering Tools\nThe tools we defined for our data engineering agent are the following: code_generation(), code_execution(), and documentation(). These tools are defined via the LangChain tools module. These tools, which are further described on the Agents page, are called upon through the engineering agent via its defined system prompt."
  },
  {
    "objectID": "6_tools.html#frameworks",
    "href": "6_tools.html#frameworks",
    "title": "Tools",
    "section": "Frameworks",
    "text": "Frameworks\nMention streamlit interface with in chat loggers"
  },
  {
    "objectID": "6_tools.html#version-control-and-cicd",
    "href": "6_tools.html#version-control-and-cicd",
    "title": "Tools",
    "section": "Version Control and CI/CD",
    "text": "Version Control and CI/CD\nFor version control, we used Git and hosted our code on GitHub. Our Github repository contains all the code for our pipeline, as well as milestone documents, working files from previous iterations, and a README file that provides an overview of the project. We also used several branches in our repository to manage different features and iterations of our project.\n \n\nNext Section: Evaluation of Effectiveness\n\n\n\nReferences:\n\\({^1}\\) LangChain\n\\({^2}\\) Bedrock\n\\({^3}\\) Streamlit\n\\({^4}\\) Github"
  },
  {
    "objectID": "3_rag.html",
    "href": "3_rag.html",
    "title": "Retrieval-Augmented Generation (RAG)",
    "section": "",
    "text": "Within our multi-agent pipeline, we have implemented a Retrieval-Augmented Generation (RAG) system to act as the question-answering portion of the application. The RAG system is designed to read the user’s query, retrieve the sources needed in the vectorstore to answer the question, and consolidate the sources to provide an accurate answer. This is especially important for stock market data, as the information is constantly changing, includes exact values, and is often time-sensitive. With a RAG we’re able to avoid any type of hallucination and provide the most accurate and up-to-date information to the user.\nThe RAG system is built using the LangChain framework, which allows us to easily integrate various components such as the vectorstore, retriever, and LLM. The vectorstore was created using FAISS, a popular library for efficient similarity search and clustering of dense vectors. The embeddings used to create the vectorstore were generated from Hugging Face’s intfloat/e5-small-v2 model, which is lightweight and efficient with both semantic understanding and numeric retrieval capabilities. Our retriever is uses the maximum marginal relevance (MMR) algorithm to ensure that the retrieved documents are both relevant and diverse, which is crucial for providing comprehensive answers to user queries. Additionally, we have set k in the retriever to 30 since we found the RAG to suffer in retrieving the correct documents when k was lower, mainly due to the fact the RAG struggles with filtering data by specific dates and names. Lastly, the LLM used for answer generation is Claude 3.5 Sonnet from Anthropic, which is a state-of-the-art model known for its reasoning capabilities and ability to generate coherent and contextually relevant responses with being too wordy.\nWhen it came to creating the structure of the RAG, we also need to take into consideration the chat history and how to best utilize it. We decided that rather than creating a RAG chain, we would seperate it by question answer chain and document retrieval. We first combine the user query and the chat history into a single string, which is then passed to the retriever to find the most relevant documents. The retrieved documents are then passed to the question answer chain, (made up of the llm, system prompt, and documents) to generate a response. This approach allows us to maintain the context of the conversation while ensuring that the RAG system can effectively retrieve and utilize the most relevant information.\nThe RAG system is able to provide a cohesive answer, either directly based on the retrived documents or a statement that not enough data was found, back to the supervisor agent to be sent to the user. The system is build such that the supervisor agent simple transfers the RAG’s output to the user instead of augmenting any additonal details to avoid any type of hallucination.\n \n\nNext Section: AI Agents\n\n\n\nReferences:\n\\({^1}\\) LangChain\n\\({^2}\\) Hugging Face\n\\({^3}\\) FAISS"
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Our Stock Market LLM Assistant will be designed to provide insights on stock market data using a multi-agent system. The application leverages data engineering and RAG (Retrieval-Augmented Generation) agents to automate data collection, processing, and query handling. The data agent gathers stock market data via APIs, and cleans and stores it using FAISS. The engineering agent generates visualizations, executes code, and documents processes. The RAG agent answers user queries, analyzes stock trends, compares tickers, and generates customized datasets and visualizations. Our fully-functioning generative AI model will be contained in a user-friendly application which offers an interactive and intuitive experience for users seeking stock market insights.\nOur goal for this assistant is to create an application that is accessible to a wide range of users; from novice investors to experienced traders. The streamlit interface acts like a chat bot that allows users to ask questions about stock market data and receive real-time insights. Automated report generation, especially in finance, is a powerful and needed tool in the corporate world right now and we believe this project to serve as a stepping stone into more sophisticated iterations in the future for others to build upon."
  },
  {
    "objectID": "1_introduction.html#points-of-analysis",
    "href": "1_introduction.html#points-of-analysis",
    "title": "Introduction",
    "section": "Points of Analysis",
    "text": "Points of Analysis\n\nData Science Question\n\nHow can we build a Stock Market LLM Assistant leveraging multi-agent systems to deliver users with an efficient, intuitive interface for comprehensive financial and technical stock market analysis?\n\n\n\nProject Questions:\n\nWhat sources of data are the most relevant to provide users with accurate and timely stock market insights?\n\n\nCan we create a multi-agent system that automates data collection, processing, and query handling for stock market analysis?\n\n\nWhat models and techniques can we implement for RAG to optimize query-to-context retrieval and response generation?\n\n\nCan we implement tools that allow users to generate customized financial reports and visualizations?\n\n\nShould we consider AI ethics and bias in our model to ensure uninformed decision-making?\n\n\nWhat are future improvements we can make to enhance the performance and usability of the Stock Market LLM Assistant?\n\n \n\nNext Section: Data Sources\n\n\n\n\nReferences:\n\\({^1}\\) Gamage, P. (2026). Applied Time Series for Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Speeding up Stock Market Insights with Generative AI",
    "section": "",
    "text": "M.S. in Data Science and Analytics\n    \n\n    Applied Generative AI for Developers\n    Final Project\n    Speeding up Stock Market Insights with Generative AI\n    A Multi-Agent System for Stock Market Analysis\n    By: Shriya Chinthak and Maria Agustina Zuckerberg"
  },
  {
    "objectID": "4_agents.html",
    "href": "4_agents.html",
    "title": "AI Agents",
    "section": "",
    "text": "Project Pipeline\nOur Stock Market LLM Assistant can be described as a hierarchical multi-agent system, where the supervisor agent is meant to delegate tasks between its sub agents and consolidate their outputs to present to the user. Below is a description of the agents, what we used to build them, and how they interact with each other."
  },
  {
    "objectID": "4_agents.html#supervisor-agent",
    "href": "4_agents.html#supervisor-agent",
    "title": "AI Agents",
    "section": "Supervisor Agent",
    "text": "Supervisor Agent\nThe Supervisor Agent is the main orchestrator of the entire system. It is responsible for managing the flow of information between the user and the various sub-agents; data, RAG, and engineering. The main technology used to build the Supervisor Agent is LangChain’s StateGraph package, which defines the Supervisor Agent as a state machine, with the sub agents acting as nodes in the graph. Using Claude 3.5 Sonnet from Anthropic, the Supervisor Agent is able to read the user’s query and determine, based on our system prompt, which node to invoke. Since queries will either be Q&A style responses or analytical requests, the nodes are two distinct paths: one for the RAG Agent and one for the Engineering Agent, where both will first invoke the Data Gathering Agent to collect the most up-to-date data before proceeding.\nAdditionally, the Supervisor Agent initializes all nodes in the graph with chat memory, setting context for each node. We utilized LangChain’s storage methods InMemoryStore() and InMemorySaver() to store the chat history for each chat instance. This allows for the subagents to have access to any previous questions and answers in the current chat to augment the current query to be answered. For example, if the user asks a question like “What was the price of AAPL on the previous day?”, the Supervisor Agent will be able to access the previous day’s data from the chat history and pass it to the RAG Agent to answer the question.\nLastly, if for any reason, the sug agents run into errors, the Supervisor Agent is able to catch these errors and gracefully handle them by providing a fallback answer to the user.\nThus, these three components of the Supervisor Agent allow it to efficiently manage the flow of information between the user and the sub-agents, without the need of human intervention or any hard coding for its decision making."
  },
  {
    "objectID": "4_agents.html#data-gathering-agent",
    "href": "4_agents.html#data-gathering-agent",
    "title": "AI Agents",
    "section": "Data Gathering Agent",
    "text": "Data Gathering Agent\nThe Data Gathering Agent is an autonomous agent designed to collect both stock price and news data based on the tickers specified in the user’s query. To achieve this, the agent leverages two tools: gather_data() and data_to_vectorstore(). Regardless of the user’s request type, this agent is always invoked as a first step following specific instructions. First, the LLM identifies the tickers mentioned in the query and compiles them into a list to be passed into the gather_data() tool. Then, it gathers the most up-to-date price and news data for the defined tickers using the python package Finnhub that uses an API to access Yahoo Finance data. Finally, if new data has been successfully retrieved, the agent either creates a new vectorstore or updates an existing one, ensuring that the information is ready to be used by the next agents in the process: the RAG Agent and the Data Engineering Agent."
  },
  {
    "objectID": "4_agents.html#rag-agent",
    "href": "4_agents.html#rag-agent",
    "title": "AI Agents",
    "section": "RAG Agent",
    "text": "RAG Agent\nThe RAG Agent is responsible for answering user queries by retrieving relevant information from the vectorstore and generating coherent responses. Unlike the data and engineering agents, the RAG Agent does not have any tools associated with it. Instead, it runs the create_rag_chain() function, which initializes the sentence embeddings, loads the vectorstore, creates the retriever, instantiates the prompt using ChatPromptTemplate, uses the LLM to rewrite the query based on the chat history, and retrieves the relevant documents provide an answer using the question-answer chain. The answer is then passed back to the Supervisor Agen to push to the user without any additional augmentation.\nThe structure of our RAG agent allows for efficient retrieval and effectively uses chat history to provide accurate answers. The RAG Agent is designed to be flexible and can be easily updated with new data or models as needed."
  },
  {
    "objectID": "4_agents.html#engineering-agent",
    "href": "4_agents.html#engineering-agent",
    "title": "AI Agents",
    "section": "Engineering Agent",
    "text": "Engineering Agent\nThe Engineering Agent is responsible for generating any code requested from the user, executes that code, and generates documentation about the code and its outputs. This agent is built using the LangChain framework create_react_agent and has the following tools associated with it: code_generation(), code_execution(), and documentation(). When the user requests for any sort of code, report, or even mathematical calculation, the Supervisor Agent will invoke the Engineering Agent. The Engineering Agent will then use the code_generation() tool to generate the code needed to complete the task. Once the code is generated, it will be passed to the code_execution() tool to run the code and generate any outputs. If there are any errors during the execution, the agent will edit the code back in the code_generation() tool and rerun the code_execution() tool until the code runs correctly. Finally, the outputs are passed to the documentation() tool to create a report about the code, the steps the agent took to create the code, and its outputs along with any requested analysis. The report is then passed back to the Supervisor Agent to be summarized to the user as well as saved in a local directory and S3 bucket for the user to download.\n\nThus, our multi-agent system is designed to be flexible, efficient, and user-friendly. The Supervisor Agent orchestrates the entire process, while the Data Gathering Agent collects the necessary data, the RAG Agent retrieves and generates answers, and the Engineering Agent handles code generation and documentation. This structure allows for seamless interaction between the agents and ensures that users receive accurate and timely responses to their queries.\nNext, we will discuss the specific models and technologies used in our Stock Market LLM Assistant project.\n \n\nNext Section: Models and Technologies Used\n\n\n\nReferences:\n\\({^1}\\)"
  },
  {
    "objectID": "5_model.html",
    "href": "5_model.html",
    "title": "Models",
    "section": "",
    "text": "To create our Stock Market LLM Assistant, we utilized a variety of technologies and frameworks. Below is a summary of the key components as well as the decision making process behind our choices.\n\n\nSince our project is a multi-agent system, we are using LLMs for all the agents. However, we quickly realized that in order to deliver the best results for our users, we needed to use the best model we had access to for all the agents. Thus, we used Claude 3.5 Sonnet for all agents in our system. Claude 3.5 Sonnet is a state-of-the-art LLM that has been trained on a diverse range of data, making it well-suited for a variety of tasks, including code generation, data analysis, and natural language understanding, summarization, and more. Through our testing, we found that other Bedrock models such as Nova and Titan 2 were not as effective for our use cases as Claude 3.5 Sonnet.\nAt this time, we do not have the resources to use other LLM’s such as OpenAI’s GPT-4, but that can be a future consideration.\n\n\n\nHaving a good embedding model is crucial for the performance of our RAG agent. We initially used the HuggingFace embedding model BAAI/bge-small-en for our RAG agent since we had used it in previous projects for this class. However, we quickly realized that this embedding model was not able to parse both semantic textual data and highly formatted tabular data in the form of json structures. Since our data contains both text data from the stock news articles and tabular data from the stock market data, we needed a model that could handle both to aid in the retrieval process.\nAfter some research, we found that the HuggingFace embedding model intfloat/e5-small-v2 was the best model for our use case. This model, unlike the previous one, is trained for general-purpose retrieval and ranking (multi-task), while BAAI/bge-small-en is mainly trained for English text understanding. intfloat/e5-small-v2 performs best for instruction-following tasks, so our labeled json data was able to be parsed and embedded correctly.\n\n\n\nAfter testing the performance of our sub agents and combined pipeline, we chose not to implement any inference optimization techniques such as quantization, distillation, or multi-adapter swapping. This was mainly due to the fact that our models were already performing well and we did not want to risk losing any performance by implementing these techniques. However, this is something we will consider in the future as we continue to improve our assistant.\n \n\nNext Section: Tools and Frameworks\n\n\n\n\n\n\\({^1}\\) LangChain\n\\({^2}\\) Hugging Face\n\\({^3}\\) Bedrock\n\\({^4}\\) FAISS"
  },
  {
    "objectID": "5_model.html#technologies-used",
    "href": "5_model.html#technologies-used",
    "title": "Models",
    "section": "",
    "text": "To create our Stock Market LLM Assistant, we utilized a variety of technologies and frameworks. Below is a summary of the key components as well as the decision making process behind our choices.\n\n\nSince our project is a multi-agent system, we are using LLMs for all the agents. However, we quickly realized that in order to deliver the best results for our users, we needed to use the best model we had access to for all the agents. Thus, we used Claude 3.5 Sonnet for all agents in our system. Claude 3.5 Sonnet is a state-of-the-art LLM that has been trained on a diverse range of data, making it well-suited for a variety of tasks, including code generation, data analysis, and natural language understanding, summarization, and more. Through our testing, we found that other Bedrock models such as Nova and Titan 2 were not as effective for our use cases as Claude 3.5 Sonnet.\nAt this time, we do not have the resources to use other LLM’s such as OpenAI’s GPT-4, but that can be a future consideration.\n\n\n\nHaving a good embedding model is crucial for the performance of our RAG agent. We initially used the HuggingFace embedding model BAAI/bge-small-en for our RAG agent since we had used it in previous projects for this class. However, we quickly realized that this embedding model was not able to parse both semantic textual data and highly formatted tabular data in the form of json structures. Since our data contains both text data from the stock news articles and tabular data from the stock market data, we needed a model that could handle both to aid in the retrieval process.\nAfter some research, we found that the HuggingFace embedding model intfloat/e5-small-v2 was the best model for our use case. This model, unlike the previous one, is trained for general-purpose retrieval and ranking (multi-task), while BAAI/bge-small-en is mainly trained for English text understanding. intfloat/e5-small-v2 performs best for instruction-following tasks, so our labeled json data was able to be parsed and embedded correctly.\n\n\n\nAfter testing the performance of our sub agents and combined pipeline, we chose not to implement any inference optimization techniques such as quantization, distillation, or multi-adapter swapping. This was mainly due to the fact that our models were already performing well and we did not want to risk losing any performance by implementing these techniques. However, this is something we will consider in the future as we continue to improve our assistant.\n \n\nNext Section: Tools and Frameworks\n\n\n\n\n\n\\({^1}\\) LangChain\n\\({^2}\\) Hugging Face\n\\({^3}\\) Bedrock\n\\({^4}\\) FAISS"
  }
]